{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05aba0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Contents:\n",
    "-data intialization from csv file of words &\n",
    " build data structure to house info moving forward\n",
    "-write to .json formats\n",
    "-read from existing .json formats\n",
    "-get list of words to find\n",
    "-scrape BBC\n",
    "-scrape voiceTV\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ebdc6804",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4, requests, sys, codecs, urllib.request, re\n",
    "from bs4 import SoupStrainer\n",
    "from bs4.element import Comment\n",
    "import random\n",
    "#import pythainlp\n",
    "from pythainlp.tokenize import word_tokenize, sent_tokenize\n",
    "import string\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pprint\n",
    "import translators as ts\n",
    "\n",
    "pp = pprint.PrettyPrinter()\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', 0)\n",
    "\n",
    "path = \"/Users/elyebliss/Desktop/Vocabulary/language_learning/vocab_dfs/\"\n",
    "source_file = \"thai.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ce69b1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "##METHODS\n",
    "\n",
    "def tag_visible(element):\n",
    "    if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "        return False\n",
    "    if isinstance(element, Comment):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def text_from_html(body):\n",
    "    soup = bs4.BeautifulSoup(body, 'html.parser')\n",
    "    texts = soup.findAll(text=True)\n",
    "    visible_texts = filter(tag_visible, texts)  \n",
    "    return u\" \".join(t.strip() for t in visible_texts)\n",
    "\n",
    "\n",
    "\n",
    "user_agent = 'Mozilla/5.0 (Windows NT 6.3; WOW64; rv:50.0) Gecko/20100101 Firefox/50.0'\n",
    "headers={'User-Agent':user_agent,}\n",
    "parser = 'html.parser'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "63afba50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_known(unknown_list):\n",
    "    \n",
    "    count_got = 0\n",
    "    known_list = []\n",
    "    for word in unknown_list:\n",
    "        decision = str(input(word+\"\\nKnown =k\"))\n",
    "        if decision =='k':\n",
    "            known_list.append(word)\n",
    "            count_got +=1\n",
    "            print(\"got \"+str(count_got))\n",
    "        elif decision=='q':\n",
    "            break\n",
    "        try:\n",
    "            print(ts.google(word))\n",
    "        except:\n",
    "            print('cant find')\n",
    "    return known_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "117b26f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_text(webpage,start=None,stop=None,\\\n",
    "                print_word_lvl=False,percent_threshold=None,\\\n",
    "               return_percent=False):\n",
    "\n",
    "\n",
    "    try:\n",
    "        request=urllib.request.Request(webpage,None,headers) #The assembled request\n",
    "        response = urllib.request.urlopen(request)\n",
    "        data = response.read()\n",
    "        contents = text_from_html(data)\n",
    "\n",
    "        known_array = []\n",
    "        unk_array = []\n",
    "        contents_array = sent_tokenize(contents)\n",
    "\n",
    "        if (start is not None) and (stop is not None):\n",
    "            contents_array=contents_array[max(start,0):min(stop,len(contents_array))]\n",
    "\n",
    "        disallowed_words = set()\n",
    "\n",
    "        total_words = 0\n",
    "        unknown_words = 0\n",
    "        \n",
    "        lines = []\n",
    "        unknowns = []\n",
    "        for line in contents_array:\n",
    "\n",
    "\n",
    "            tokenized = word_tokenize(line)\n",
    "\n",
    "\n",
    "            add_line = True\n",
    "            line_total = 0\n",
    "            line_unks = 0\n",
    "\n",
    "            unk_str = \"\"\n",
    "            for word in tokenized:\n",
    "                \n",
    "                total_words +=1\n",
    "                line_total +=1\n",
    "\n",
    "                if bool(re.search('[\\u0E00-\\u0E7F]+', word, flags=re.UNICODE)) and not ((word in vocab['white_listed']) or\\\n",
    "                                                            (word in vocab['black_listed'])):\n",
    "\n",
    "                        unk_str += '\"'+word+'\"'+\", \"\n",
    "                        if not percent_threshold:\n",
    "                            add_line = False\n",
    "                        disallowed_words.add(word)\n",
    "                        unknown_words +=1\n",
    "                        line_unks +=1\n",
    "\n",
    "            if percent_threshold:\n",
    "                if line_total>0:\n",
    "                    if (1-(line_unks/line_total))<percent_threshold:\n",
    "                        add_line = False\n",
    "            if add_line:\n",
    "                known_array.append(line)\n",
    "                unk_array.append(\"...\")\n",
    "            else:\n",
    "                known_array.append(\"...\")\n",
    "                unk_array.append(line)\n",
    "\n",
    "            if len(unk_str)>0:\n",
    "                unk_str = unk_str[0:len(unk_str)-2]\n",
    "\n",
    "            unknowns.append(unk_str)\n",
    "\n",
    "        if print_word_lvl and total_words>0:\n",
    "            print(\"word-level % known = \"+str((1-(unknown_words/total_words))*100))\n",
    "        if return_percent and total_words>0:\n",
    "            return (1-(unknown_words/total_words))*100\n",
    "        return_pd = pd.DataFrame(list(zip(known_array,unk_array,unknowns)))\n",
    "        return_pd.columns = [\"knowns\",\"unknowns\",\"unk_words\"]\n",
    "\n",
    "        \n",
    "\n",
    "        with open(path+'unknown_thai_list.txt',\"w\") as outfile:\n",
    "            outfile.write(str(list(disallowed_words)))\n",
    "\n",
    "        return return_pd\n",
    "\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "826dae68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2288\n",
      "2518\n"
     ]
    }
   ],
   "source": [
    "\"\"\"read from existing .json formats\n",
    "white-listed\n",
    "uncovered \n",
    "2288\n",
    "2518\n",
    "\"\"\"\n",
    "with open(path+source_file, \"r\") as path_in:\n",
    "    vocab = json.loads(path_in.read())\n",
    "vocab['white_listed'] = set(vocab['white_listed'])\n",
    "vocab['black_listed'] = set(vocab['black_listed'])\n",
    "print(len(vocab['white_listed'])) \n",
    "\n",
    "#get 4k freq word info:\n",
    "with open(path+'th_freq.csv','r') as infile:\n",
    "    freq_df = pd.read_csv(infile)\n",
    "uncovered = []\n",
    "for word in freq_df.word:\n",
    "    if word not in vocab['white_listed']:\n",
    "        uncovered.append(word)\n",
    "print(len(uncovered))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "004a939d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking BBC Thai\n",
      "Checking VoiceTV\n"
     ]
    }
   ],
   "source": [
    "pages = []\n",
    "titles = []\n",
    "\n",
    "##BBC\n",
    "print(\"Checking BBC Thai\")\n",
    "parser = 'html.parser'  # or 'lxml' (preferred) or 'html5lib', if installed\n",
    "request=urllib.request.Request('https://www.bbc.com/thai/topics/cjgn73g98rqt',None,headers)\n",
    "resp = urllib.request.urlopen(request)\n",
    "soup = bs4.BeautifulSoup(resp, parser, from_encoding=resp.info().get_param('charset'))\n",
    "\n",
    "\n",
    "for link in soup.find_all('a', href=True):\n",
    "    if 'https://www.bbc.com/thai/thailand' in str(link['href']):\n",
    "        pages.append(str(link['href']))\n",
    "        request=urllib.request.Request(str(link['href']),None,headers) #The assembled request\n",
    "\n",
    "        response = urllib.request.urlopen(request)\n",
    "        data = response.read()\n",
    "        contents = text_from_html(data)\n",
    "        if bool(re.search(\"(?<=ยอดนิยม หน้าแรก ประเทศไทย ต่างประเทศ วิทยาศาสตร์ สุขภาพ โควิด-19 วิดีโอ ยอดนิยม).{30}\",contents)):\n",
    "            titles.append(re.findall(\"(?<=ยอดนิยม หน้าแรก ประเทศไทย ต่างประเทศ วิทยาศาสตร์ สุขภาพ โควิด-19 วิดีโอ ยอดนิยม).{30}\",contents)[0].strip())\n",
    "\n",
    "    elif '/thai/thailand' in str(link['href']):\n",
    "        pages.append('https://www.bbc.com'+str(link['href']))\n",
    "\n",
    "##VoiceTV\n",
    "print(\"Checking VoiceTV\")\n",
    "parser = 'html.parser'  # or 'lxml' (preferred) or 'html5lib', if installed\n",
    "request=urllib.request.Request('https://www.voicetv.co.th/topic/%E0%B8%81%E0%B8%B2%E0%B8%A3%E0%B9%80%E0%B8%A1%E0%B8%B7%E0%B8%AD%E0%B8%87',None,headers)\n",
    "resp = urllib.request.urlopen(request)\n",
    "soup = bs4.BeautifulSoup(resp, parser, from_encoding=resp.info().get_param('charset'))\n",
    "\n",
    " \n",
    "for link in soup.find_all('a', href=True):\n",
    "    str_page = 'https://www.voicetv.co.th'+str(link['href'])\n",
    "    if '/read/' in str(link['href']) and str_page not in pages:\n",
    "        pages.append(str_page)\n",
    "        request=urllib.request.Request(str_page,None,headers) #The assembled request\n",
    "\n",
    "        response = urllib.request.urlopen(request)\n",
    "        data = response.read()\n",
    "        contents = text_from_html(data)\n",
    "        titles.append(re.findall(\"(?<=  :).{30}\",contents)[0].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "411c6c9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1565"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load prev corpus and add\n",
    "known_corpus = set()\n",
    "with open(path+\"all_known_thai_lines.txt\",\"r\",encoding='utf-8') as infile:\n",
    "    for line in infile.read().split('\\n'):\n",
    "        known_corpus.add(line)\n",
    "len(known_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bb6ee0f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percent all known= 0.16391184573002754\n"
     ]
    }
   ],
   "source": [
    "#Find % of all sentences of news\n",
    "knowns = []\n",
    "all_lines = 0\n",
    "\n",
    "known_percents = []\n",
    "\n",
    "for webpage in pages:\n",
    "    \n",
    "    test_df = filter_text(webpage)\n",
    "    all_lines += len(list(test_df.knowns))\n",
    "    \n",
    "    known_percents.append(filter_text(webpage,return_percent=True))\n",
    "    \n",
    "    for item in list(test_df.knowns[test_df.knowns != '...']):\n",
    "        knowns.append(item)\n",
    "percent = len(knowns)/float(all_lines)\n",
    "print(\"percent all known= \"+str(percent))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1fa348ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['รอติดตามตอนต่อไป    //// Topic การเมือง Author กองบรรณาธิการวอยซ์ออนไลน์ 51505 Article 1192 Video 10 Blog MOST VIEWED READ WATCH ABOUT FAQ CONTACT TERM OF USE SCHEDULE      ',\n",
       " 'ถึงอย่างนั้นเขายังคิดถึงและต้องการกลับเมืองไทยซึ่งเป็นประเทศบ้านเกิดที่เขารัก ที่เขาเคยมี \"ความฝัน\" \"ผมฝันอยากเป็นตำรวจที่ดีครับ แต่มันยากมากครับ\" ',\n",
       " 'ปัจจุบันขายใบละ 100 บาท ชุด 5 ใบ 600-800 บาท ชุด 10 ใบ ขายตั้งแต่ 1500-3500 บาท 2.ปัญหาค่าไฟแพง ',\n",
       " 'หนึ่งวันก็อยู่ได้\" หัวหน้ารวมไทยสร้างชาติชูประยุทธ์ 23 ธันวาคม 2022 จะเกิดอะไรกับอินเดีย ',\n",
       " 'ตอนนั้นผมมีสองทางเลือกไปตรงนี้หรือสามจังหวัดภาคใต้\" ',\n",
       " 'ตนทราบดีว่าเรายังห่างจากพรรคอันดับหนึ่งอยู่พอสมควร ',\n",
       " 'ซึ่งผมเห็นว่า นายกฯ ต้องเป็น ส.ส. ต้องมาจากประชาชน ',\n",
       " 'แต่วันนี้ต้องมาอยู่ในที่ใหม่ที่ไม่เคยรู้จักมาก่อน ',\n",
       " 'เห็นไหมครับว่ามีกระบวนการช่วยเหลือทหารทั้งหมด ',\n",
       " 'พยายามทำให้เราพออยู่ได้ ให้สังคมอยู่ได้ ',\n",
       " 'คงไม่ลำบากทั้งกายและใจเหมือนทุกวันนี้ ',\n",
       " 'เมื่อวันที่ 3 มิ.ย. 2558 ก่อนหน้าที่ ',\n",
       " 'เรื่องพื้นฐานให้ประชาชนยังไม่ได้เลย ',\n",
       " 'ยังกล้าขอโอกาสไปต่อ Dec 25, 2022 ( ',\n",
       " 'ไม่มีการลงระบบ Crime ซึ่งเป็นระบบ ',\n",
       " 'แต่สามจังหวัดพื้นที่ทหารหมด ',\n",
       " 'ส่วนรายละเอียดจะเป็นอย่างไร ',\n",
       " 'ยืนยันว่าไม่เคยโทรศัพท์ไปหา ',\n",
       " 'มีหลายประเด็นที่มีข้อสงสัย ',\n",
       " 'ไม่คิดว่ามันจะเกิดกับผมเลย ',\n",
       " 'หลังต่อสู้คดีมาเกือบ 10 ปี ',\n",
       " 'ไม่ยอมลดค่า FT ให้ประชาชน ',\n",
       " 'คิดว่าไม่มีใครตรวจสอบได้ ',\n",
       " 'ผมเชื่อว่า พล.อ.ประยุทธ์ ',\n",
       " 'เพราะถ้าผมไม่ได้หลักฐาน ',\n",
       " 'ชี้เป็นพรรคใหม่ 4 เดือน ',\n",
       " 'เมื่อ 1 ชั่วโมงที่แล้ว ',\n",
       " 'ไม่ใช่อยากจับใครก็จับ ',\n",
       " 'ในฐานะนายกฯ และรัฐบาล ',\n",
       " 'แล้วจับใครไม่ได้สักคน ',\n",
       " 'ผมไม่ใช่ขนาดนั้น ',\n",
       " 'เพราะตนเองมองว่า ',\n",
       " 'นี่คือประเทศไทย ',\n",
       " 'เราต้องทำให้ได้ ',\n",
       " 'ผ่านไปกว่า 7 ปี ',\n",
       " 'ยังแก้ไม่ได้เลย ',\n",
       " 'ไม่ผิดก็กลับมา ',\n",
       " 'มันไม่อันตราย ',\n",
       " 'เพราะกรุงเทพฯ ',\n",
       " 'จะไม่ได้ไปต่อ ',\n",
       " 'อย่าไปเสี่ยง ',\n",
       " 'ยังแก้ไม่ได้ ',\n",
       " 'เพราะเจ็บมือ ',\n",
       " 'ไปร้องเรียน ',\n",
       " 'ผมระวังมาก ',\n",
       " 'ไม่ยอมแก้ ',\n",
       " 'สายสมัย ',\n",
       " 'คดีนี้ ']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sorted(list(set(knowns)),key=len,reverse=True)\n",
    "sorted(list(set(knowns).difference(known_corpus)),key=len,reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac029f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#manually remove from white\n",
    "remove_from_white = []\n",
    "\n",
    "for word in remove_from_white:\n",
    "    if word in vocab['white_listed']:\n",
    "        vocab['white_listed'].remove(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1ee450fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1613"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for item in list(set(knowns)):\n",
    "    known_corpus.add(item)\n",
    "\n",
    "with open(path+\"all_known_thai_lines.txt\",\"w\",encoding='utf-8') as outfile:\n",
    "    for line in list(set(known_corpus)):\n",
    "        outfile.write(line+'\\n')\n",
    "len(known_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2d6948c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n"
     ]
    }
   ],
   "source": [
    "websites = set()\n",
    "\n",
    "with open(path+'viewed_websites_th.txt',\"r\") as infile:\n",
    "    for line in infile.read().split('\\n'):\n",
    "        websites.add(line)\n",
    "print(len(websites))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2a643828",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3,\n",
       "  93.29805996472663,\n",
       "  \"'ศรีสุวรรณ' จ่อร้อง รมว.ศธ. เ\",\n",
       "  'https://www.voicetv.co.th/read/WUoYbxnNo'),\n",
       " (2,\n",
       "  89.58333333333334,\n",
       "  \"'ชูวิทย์' หวั่น 'คดีตู้ห่าว'\",\n",
       "  'https://www.voicetv.co.th/read/AM3NqZgha'),\n",
       " (4,\n",
       "  82.84762697751873,\n",
       "  '‘ประยุทธ์’ อวยพรปีใหม่สื่อทำเ',\n",
       "  'https://www.voicetv.co.th/read/cct3INJVX'),\n",
       " (6,\n",
       "  81.67539267015707,\n",
       "  \"รีวิว 'Collective': นักข่าวไม\",\n",
       "  'https://www.voicetv.co.th/read/bn95YMr0a'),\n",
       " (0,\n",
       "  80.78050981276787,\n",
       "  \"เลขา ครป. ซัด 'ประยุทธ์' 10 ป\",\n",
       "  'https://www.bbc.com/thai/thailand-64090269'),\n",
       " (5,\n",
       "  80.44444444444444,\n",
       "  '‘วิษณุ’แย้มประตูยุบสภาเปิดอยู',\n",
       "  'https://www.voicetv.co.th/read/zieaSbBL5'),\n",
       " (1,\n",
       "  80.44368600682594,\n",
       "  \"'เอกนัฏ' เผยคะแนนนิยม 'รวมไทย\",\n",
       "  'https://www.bbc.com/thai/thailand-64064173')]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page_titles = list(zip(range(0,len(known_percents)),known_percents,titles,pages))\n",
    "# (sorted(list(zip(range(0,len(known_percents)),known_percents)),key = lambda x: x[1],reverse=True))\n",
    "page_titles = sorted(page_titles,key = lambda x: x[1],reverse=True)\n",
    "page_titles = [item for item in page_titles if not (item[3] in websites)]\n",
    "page_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1e2e7b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "webpage = pages[3]\n",
    "if webpage in websites:\n",
    "    print(\"already scanned! choose another!\")\n",
    "websites.add(webpage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f737a6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(path+'viewed_websites_th.txt',\"w\") as outfile:\n",
    "    for line in websites:\n",
    "        outfile.write(line+'\\n')      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "141ae514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word-level % known = 93.29805996472663\n"
     ]
    }
   ],
   "source": [
    "#display() #percent_threshold=0.95\n",
    "art=filter_text(webpage,print_word_lvl=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8bce1137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n"
     ]
    }
   ],
   "source": [
    "with open(path+'unknown_thai_list.txt',\"r\") as input_file:\n",
    "    new_words = input_file.read()\n",
    "    new_words = re.sub(\"[\\n\\'\\[\\]]\",\"\",new_words)\n",
    "    new_words = new_words.split(',')\n",
    "    new_words = [line.strip() for line in new_words] #update regex\n",
    "#new_words\n",
    "print(len(new_words))\n",
    "#known_manual = get_known(new_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "efd86a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mitosheet\n",
    "unk_df = pd.DataFrame(new_words)\n",
    "unk_df.columns = ['word']\n",
    "unk_df['status'] = pd.Series(['' for word in new_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3663bb4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07673b5fd1f640d8b78e1919175b8572",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MitoWidget(analysis_data_json='{\"analysisName\": \"id-kwyuqpfwyh\", \"analysisToReplay\": null, \"code\": [], \"stepSu…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mitosheet.sheet(unk_df, analysis_to_replay=\"id-kwyuqpfwyh\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c243db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "60d5d1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_ints = [1,2,13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "77eb121a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in add_ints:\n",
    "    # Set a cell value in status\n",
    "    unk_df.at[i, 'status'] = 'k'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "71c577e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "translations = []\n",
    "for i in range(0,len(unk_df)):\n",
    "    try:\n",
    "        translations.append(ts.google(unk_df.word.iloc[i]))\n",
    "    except:\n",
    "        translations.append('')\n",
    "unk_df['translations']=translations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fe5bce81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f959c97613946ddb8779dc66f413c01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MitoWidget(analysis_data_json='{\"analysisName\": \"id-iwboyukkfp\", \"analysisToReplay\": null, \"code\": [], \"stepSu…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mitosheet.sheet(unk_df, analysis_to_replay=\"id-iwboyukkfp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "455c3cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "add_words = list(unk_df.word[unk_df['status']=='k'])\n",
    "print(len(add_words))\n",
    "for word in add_words:\n",
    "    vocab['white_listed'].add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "02a15572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2288\n"
     ]
    }
   ],
   "source": [
    "#write to .json formats\n",
    "df = vocab\n",
    "df['white_listed'] = list(df['white_listed'])\n",
    "df['black_listed'] = list(df['black_listed'])\n",
    "with open(path+source_file, \"w\") as outfile:\n",
    "    json.dump(df,outfile)\n",
    "print(len(df['white_listed']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c7f67a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2288\n"
     ]
    }
   ],
   "source": [
    "#read from existing .json formats\n",
    "with open(path+source_file, \"r\") as path_in:\n",
    "    vocab = json.loads(path_in.read())\n",
    "vocab['white_listed'] = set(vocab['white_listed'])\n",
    "vocab['black_listed'] = set(vocab['black_listed'])\n",
    "print(len(vocab['white_listed']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4f09c08d",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.voicetv.co.th/read/WUoYbxnNo\n",
      "word-level % known = 93.65079365079364\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><div id=93208e14-b4e6-4044-8f05-c7901e2075bc style=\"display:none; background-color:#9D6CFF; color:white; width:200px; height:30px; padding-left:5px; border-radius:4px; flex-direction:row; justify-content:space-around; align-items:center;\" onmouseover=\"this.style.backgroundColor='#BA9BF8'\" onmouseout=\"this.style.backgroundColor='#9D6CFF'\" onclick=\"window.commands?.execute('create-mitosheet-from-dataframe-output');\">See Full Dataframe in Mito</div> <script> if (window.commands?.hasCommand('create-mitosheet-from-dataframe-output')) document.getElementById('93208e14-b4e6-4044-8f05-c7901e2075bc').style.display = 'flex' </script> <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>knowns</th>\n",
       "      <th>unknowns</th>\n",
       "      <th>unk_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>POLITICS ECONOMICS WORLD ENTERTAINMENT WELL-BEING LOCAL NEWS BLOG VOICE PLAZA TV PROGRAMS LIVE POLITICS ECONOMICS WORLD ENTERTAINMENT WELL-BEING LOCAL NEWS BLOG VOICE PLAZA TV PROGRAMS ABOUT FAQ CONTACT TERM OF USE SCHEDULE ไม่พบผลการค้นหา</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>...</td>\n",
       "      <td>คุณกำลังอ่าน  : 'เอกนัฏ' เผยคะแนนนิยม 'รวมไทยสร้างชาติ' พุ่ง หลัง 'ประยุทธ์' ประกาศเข้าร่วมพรรค Share Tweet Share การเมือง 'เอกนัฏ' เผยคะแนนนิยม 'รวมไทยสร้างชาติ' พุ่ง หลัง 'ประยุทธ์' ประกาศเข้าร่วมพรรค Dec 25, 2022 (</td>\n",
       "      <td>\"นัฏ\", \"คะแนนนิยม\", \"พุ่ง\", \"นัฏ\", \"คะแนนนิยม\", \"พุ่ง\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>...</td>\n",
       "      <td>Last update Dec 25, 2022 12:45 )   'เอกนัฏ' เผยคะแนนนิยม 'รวมไทยสร้างชาติ' พุ่งขึ้น 7% หลัง 'ประยุทธ์' ประกาศเข้าร่วมพรรค</td>\n",
       "      <td>\"นัฏ\", \"คะแนนนิยม\", \"พุ่ง\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ชี้เป็นพรรคใหม่ 4 เดือน</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>...</td>\n",
       "      <td>มาได้ขนาดนี้ เป็นความก้าวหน้า</td>\n",
       "      <td>\"ได้ขนาด\", \"ความก้าวหน้า\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>...</td>\n",
       "      <td>ในเวลา 4 เดือน มาได้ถึงขนาดนี้ถือเป็นความก้าวหน้า ก็ขอให้กำลังใจสมาชิกและกองเชียร์ทุกคน</td>\n",
       "      <td>\"ถึงขนาด\", \"ความก้าวหน้า\", \"กองเชียร์\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ตนทราบดีว่าเรายังห่างจากพรรคอันดับหนึ่งอยู่พอสมควร</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>...</td>\n",
       "      <td>ต้องช่วยกันทำงานอีกเยอะครับ จากนี้ไป ยังจะมีการประกาศนโยบาย ผู้สมัคร และเครือข่าย ของพรรค #รวมไทยสร้างชาติ</td>\n",
       "      <td>\"เครือข่าย\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>เราต้องทำให้ได้</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ยังยืนยันว่าถึงกระแสจะดีขึ้นแต่เราเดินหน้าให้ความสำคัญกับการสร้างคน สร้างประเทศ เหนือสิ่งอื่นใด มาช่วยกัน. Topic การเมือง , พรรครวมไทยสร้างชาติ Author กองบรรณาธิการวอยซ์ออนไลน์ 51505 Article 1192 Video 10 Blog MOST VIEWED READ WATCH ABOUT FAQ CONTACT TERM OF USE SCHEDULE</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table></div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                   knowns  \\\n",
       "0      POLITICS ECONOMICS WORLD ENTERTAINMENT WELL-BEING LOCAL NEWS BLOG VOICE PLAZA TV PROGRAMS LIVE POLITICS ECONOMICS WORLD ENTERTAINMENT WELL-BEING LOCAL NEWS BLOG VOICE PLAZA TV PROGRAMS ABOUT FAQ CONTACT TERM OF USE SCHEDULE ไม่พบผลการค้นหา                                      \n",
       "1   ...                                                                                                                                                                                                                                                                                     \n",
       "2   ...                                                                                                                                                                                                                                                                                     \n",
       "3   ชี้เป็นพรรคใหม่ 4 เดือน                                                                                                                                                                                                                                                                 \n",
       "4   ...                                                                                                                                                                                                                                                                                     \n",
       "5   ...                                                                                                                                                                                                                                                                                     \n",
       "6   ...                                                                                                                                                                                                                                                                                     \n",
       "7   ...                                                                                                                                                                                                                                                                                     \n",
       "8   ...                                                                                                                                                                                                                                                                                     \n",
       "9   ...                                                                                                                                                                                                                                                                                     \n",
       "10  ...                                                                                                                                                                                                                                                                                     \n",
       "11  ...                                                                                                                                                                                                                                                                                     \n",
       "12  ตนทราบดีว่าเรายังห่างจากพรรคอันดับหนึ่งอยู่พอสมควร                                                                                                                                                                                                                                      \n",
       "13  ...                                                                                                                                                                                                                                                                                     \n",
       "14  เราต้องทำให้ได้                                                                                                                                                                                                                                                                         \n",
       "15  ยังยืนยันว่าถึงกระแสจะดีขึ้นแต่เราเดินหน้าให้ความสำคัญกับการสร้างคน สร้างประเทศ เหนือสิ่งอื่นใด มาช่วยกัน. Topic การเมือง , พรรครวมไทยสร้างชาติ Author กองบรรณาธิการวอยซ์ออนไลน์ 51505 Article 1192 Video 10 Blog MOST VIEWED READ WATCH ABOUT FAQ CONTACT TERM OF USE SCHEDULE         \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                              unknowns  \\\n",
       "0   ...                                                                                                                                                                                                                                                                                                                                                                  \n",
       "1   คุณกำลังอ่าน  : 'เอกนัฏ' เผยคะแนนนิยม 'รวมไทยสร้างชาติ' พุ่ง หลัง 'ประยุทธ์' ประกาศเข้าร่วมพรรค Share Tweet Share การเมือง 'เอกนัฏ' เผยคะแนนนิยม 'รวมไทยสร้างชาติ' พุ่ง หลัง 'ประยุทธ์' ประกาศเข้าร่วมพรรค Dec 25, 2022 (                                                                                                                                            \n",
       "2   Last update Dec 25, 2022 12:45 )   'เอกนัฏ' เผยคะแนนนิยม 'รวมไทยสร้างชาติ' พุ่งขึ้น 7% หลัง 'ประยุทธ์' ประกาศเข้าร่วมพรรค                                                                                                                                                                                                                                            \n",
       "3   ...                                                                                                                                                                                                                                                                                                                                                                  \n",
       "4   มาได้ขนาดนี้ เป็นความก้าวหน้า                                                                                                                                                                                                                                                                                                                                        \n",
       "5   วันที่ 25 ธ.ค. เอกนัฏ พร้อมพันธุ์ เลขาธิการพรรครวมไทยสร้างชาติ (รทสช.) โพสต์ข้อความผ่านเฟซบุ๊ก ระบุว่า ขอบคุณสมาชิกและทีมงาน รวมไทยสร้างชาติ                                                                                                                                                                                                                         \n",
       "6   ทุกคนครับ จากการที่เราเดินหน้าทำงานอย่างแน่วแน่และต่อเนื่องมาตลอดระยะเวลา 4 เดือนที่ผ่านมา                                                                                                                                                                                                                                                                           \n",
       "7   ตั้งแต่ประชุมใหญ่ไปเมื่อวันที่ 3 ส.ค.65 ล่าสุดผลสำรวจคะแนนนิยมทางการเมืองของนิด้าโพลระหว่างวันที่ 17-22 ธ.ค. ก่อนที่ท่านนายกฯ #ลุงตู่จะประกาศเจตนารมย์เข้าร่วมพรรค #รวมไทยสร้างชาติ ไปเมื่อวันที่ 23 ธ.ค. คะแนนนิยมของพรรคเพิ่มขึ้นจากคราวที่แล้วมากที่สุดแตะ 7% (อยู่ใน top 3) ส่วนตัวท่านนายกฯ ลุงตู่เพิ่มขึ้นเป็น 14% เท่าที่ผมได้ประเมินดูจากการสำรวจที่ทำเอง    \n",
       "8   “หลัง” จากที่ลุงตู่ประกาศออกมาชัดเจน                                                                                                                                                                                                                                                                                                                                 \n",
       "9   คะแนนนิยมของทั้ง #ลุงตู่ และพรรคฯ เพิ่มขึ้นอีก                                                                                                                                                                                                                                                                                                                       \n",
       "10  โดยเฉพาะของพรรคฯ เพิ่มขึ้นมากพอสมควรเกือบเท่าตัว (จาก7% ของนิด้า) เราเป็นพรรคใหม่                                                                                                                                                                                                                                                                                    \n",
       "11  ในเวลา 4 เดือน มาได้ถึงขนาดนี้ถือเป็นความก้าวหน้า ก็ขอให้กำลังใจสมาชิกและกองเชียร์ทุกคน                                                                                                                                                                                                                                                                              \n",
       "12  ...                                                                                                                                                                                                                                                                                                                                                                  \n",
       "13  ต้องช่วยกันทำงานอีกเยอะครับ จากนี้ไป ยังจะมีการประกาศนโยบาย ผู้สมัคร และเครือข่าย ของพรรค #รวมไทยสร้างชาติ                                                                                                                                                                                                                                                           \n",
       "14  ...                                                                                                                                                                                                                                                                                                                                                                  \n",
       "15  ...                                                                                                                                                                                                                                                                                                                                                                  \n",
       "\n",
       "                                                                                  unk_words  \n",
       "0                                                                                            \n",
       "1   \"นัฏ\", \"คะแนนนิยม\", \"พุ่ง\", \"นัฏ\", \"คะแนนนิยม\", \"พุ่ง\"                                   \n",
       "2   \"นัฏ\", \"คะแนนนิยม\", \"พุ่ง\"                                                               \n",
       "3                                                                                            \n",
       "4   \"ได้ขนาด\", \"ความก้าวหน้า\"                                                                \n",
       "5   \"นัฏ\", \"พันธุ์\", \"เลขาธิการ\", \"(รท\", \"สช.\"                                               \n",
       "6   \"แน่วแน่\"                                                                                \n",
       "7   \"ส.ค.\", \"ล่าสุด\", \"คะแนนนิยม\", \"นิด้า\", \"โพล\", \"ตู่\", \"รมย์\", \"คะแนนนิยม\", \"แตะ\", \"ตู่\"  \n",
       "8   \"ตู่\"                                                                                    \n",
       "9   \"คะแนนนิยม\", \"ตู่\"                                                                       \n",
       "10  \"เท่าตัว\", \"นิด้า\"                                                                       \n",
       "11  \"ถึงขนาด\", \"ความก้าวหน้า\", \"กองเชียร์\"                                                   \n",
       "12                                                                                           \n",
       "13  \"เครือข่าย\"                                                                              \n",
       "14                                                                                           \n",
       "15                                                                                           "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(webpage)\n",
    "filter_text(webpage,print_word_lvl=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04bfa91",
   "metadata": {},
   "source": [
    "##END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "db36cdb8",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "daily catch in word count:\n",
      "['เชิญ', 'ยิ้ม', 'ยินดี', 'รอย', 'เอกสาร', 'สนิท', 'กระดาษ', 'คอ', 'โรงพยาบาล', 'หิน', 'จด', 'ปืน', 'โต๊ะ', 'เสื้อผ้า', 'เลือด', 'สิทธิ์', 'นักเขียน', 'สนุกสนาน', 'หัวเราะ', 'หู', 'ยิง', 'สวยงาม', 'ล่า', 'แขก', 'ดอกไม้', 'ที่นั่ง', 'เสือ', 'ปกป้อง', 'รับประทาน', 'ไอ', 'สนุก', 'ลูกชาย', 'สหรัฐ', 'นิ้ว', 'ไอ้', 'รถไฟ', 'เล็กน้อย', 'โปรด', 'คะแนน', 'กรุณา', 'โรงแรม', 'จดหมาย', 'ขับ', 'ดื่ม', 'หอม', 'ระมัดระวัง', 'ฤดู', 'ฟัน', 'เท้า', 'หล่อน', 'วางแผน', 'ลับ', 'ลอย', 'ยกเว้น', 'ลูกค้า', 'ริม', 'ม้า', 'คอมพิวเตอร์']\n"
     ]
    }
   ],
   "source": [
    "#Upload from download\n",
    "with open(path+'known_thai_list.txt',\"r\") as input_file:\n",
    "    new_words = input_file.read().split(',')\n",
    "    print(\"daily catch in word count:\")\n",
    "    print(new_words)\n",
    "for line in new_words:   \n",
    "    vocab['white_listed'].add(line.replace(\"'\",\"\").strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "0141a9c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Nong\n"
     ]
    }
   ],
   "source": [
    "\n",
    "wyw_text = 'สวัสดีครับน้อง'\n",
    "print(ts.google(wyw_text))\n",
    "#print(ts._google.language_map)\n",
    "#print(ts.google(wyw_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "67b1cbb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "today's catch % of corpus:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1.200344766395417%'"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get after when adding in new words\n",
    "print(\"today's catch % of corpus:\")\n",
    "str(100*(percent-prev_percent))+\"%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3258e547",
   "metadata": {},
   "outputs": [],
   "source": [
    "#output newly knowns\n",
    "with open(path+\"new_known_thai_lines.txt\",\"w\") as outfile:\n",
    "    for line in list(set(knowns).difference(prev_knowns)):\n",
    "        outfile.write(line+'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "1a9fb3c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity check. Current vocab size:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1462"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Sanity check. Current vocab size:\")\n",
    "len(vocab['white_listed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "564b37a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "to do:\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9799280",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "999"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##INPUT-OUTPUT\n",
    "#vocab list:\n",
    "with open(\"/Users/elyebliss/Desktop/Vocabulary/vocab_dfs/thai_whitelisted.csv\",\"r\") as infile:\n",
    "    whitelisted_lemmas = infile.read()\n",
    "\n",
    "\n",
    "##VARIABLES\n",
    "vocab_all = set()\n",
    "\n",
    "\n",
    "for line in whitelisted_lemmas.split('\\n'):\n",
    "    if len(line) > 0:\n",
    "        \n",
    "        vocab = line.strip()\n",
    "        vocab_all.add(vocab)\n",
    "        \n",
    "len(whitelisted_lemmas.split('\\n'))            \n",
    "#pp.pprint(vocab_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e37bfb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {}\n",
    "vocab['white_listed'] = list(vocab_all)\n",
    "vocab['black_listed'] = []"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
