{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05aba0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Get articles/texts, and filter only sentences with white-listed vocab OR named entities.\n",
    "\n",
    "Contents:\n",
    "-load packages\n",
    "-create helper functions\n",
    "-data intialization from csv file of words &\n",
    " extend vocab set\n",
    "-build data structure to house info moving forward\n",
    "-write to .json formats\n",
    "-read from existing .json formats\n",
    "-get white-listed of words and inflections\n",
    "-scrape journale en francais facile\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ebdc6804",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4, requests, sys, codecs, urllib.request, re\n",
    "from bs4 import SoupStrainer\n",
    "from bs4.element import Comment\n",
    "import random\n",
    "import string\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import mlconjug3\n",
    "import sklearn\n",
    "import goslate\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter()\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', 0)\n",
    "import mitosheet\n",
    "import translators as ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b5dfc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noun(add_word):\n",
    "\n",
    "    if add_word[len(add_word)-1] != 's' and add_word[len(add_word)-1] != 'x':\n",
    "\n",
    "        out_word = add_word + 's'\n",
    "\n",
    "        return out_word\n",
    "    else:\n",
    "        return add_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ce69b1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "##METHODS\n",
    "\n",
    "def tag_visible(element):\n",
    "    if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "        return False\n",
    "    if isinstance(element, Comment):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def text_from_html(body):\n",
    "    soup = bs4.BeautifulSoup(body, 'html.parser')\n",
    "    texts = soup.findAll(text=True)\n",
    "    visible_texts = filter(tag_visible, texts)  \n",
    "    return u\" \".join(t.strip() for t in visible_texts)\n",
    "\n",
    "\n",
    "\n",
    "user_agent = 'Mozilla/5.0 (Windows NT 6.3; WOW64; rv:50.0) Gecko/20100101 Firefox/50.0'\n",
    "headers={'User-Agent':user_agent,}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f2ac063",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_known(unknown_list):\n",
    "    \n",
    "    count_got = 0\n",
    "    known_list = []\n",
    "    count_checked = 0\n",
    "    for word in unknown_list:\n",
    "        count_checked += 1\n",
    "        decision = str(input(word+\"\\nKnown =k\"))\n",
    "        if decision =='k':\n",
    "            known_list.append(word)\n",
    "            count_got +=1\n",
    "            print(\"got \"+str(count_got))\n",
    "        elif decision=='q':\n",
    "            break\n",
    "        print(str(len(unknown_list)-count_checked)+' remaining')\n",
    "    return known_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "021f0dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_out(out_dict,prev_learned,eb_unk_in,eb_unk_m_in,examples):\n",
    "    \"\"\"\n",
    "    out_dict = out\n",
    "    prev_learned = eb_learned\n",
    "    eb_unk_in =  eb_unk\n",
    "    eb_unk_m_in = eb_unk_morph\n",
    "    examples = eb_unk_examples\n",
    "    \n",
    "    decisions:\n",
    "    k = word known, remove from eb_unk and eb_unk_morph\n",
    "        and add to eb_learned\n",
    "        \n",
    "    m = remove morph from eb_unk and eb_unk_morph \n",
    "    \n",
    "    a = add example to eb_unk_examples\n",
    "    q = quit for day\n",
    "    \"\"\"\n",
    "    for word in out_dict.keys():\n",
    "        print(\"Lemma: \"+word)\n",
    "        print(\"Morph: \"+str(out_dict[word]['morph']))\n",
    "        \n",
    "        if len(examples[word])>0:\n",
    "            print(\"Previous examples: \")\n",
    "            for sent in examples[word]:\n",
    "                print(\" - \"+sent)\n",
    "                    \n",
    "        print(\"New examples: \")\n",
    "        \n",
    "        for sent in out_dict[word]['sents']:\n",
    "            if sent not in examples[word]:\n",
    "                print(sent)\n",
    "                decision = str(input(\"options : k,m,a,q\"))\n",
    "                if decision =='k':\n",
    "                    try:\n",
    "                        examples[word].append(sent) #by default adds examples\n",
    "                        prev_learned[word] = eb_unk_in[word].copy()\n",
    "                        del eb_unk_in[word] #remove lemma\n",
    "\n",
    "                        i = 0\n",
    "                        while i < len(eb_unk_m_in):\n",
    "\n",
    "                            if eb_unk_m_in[list(eb_unk_m_in)[i]] == word:\n",
    "                               del eb_unk_m_in[list(eb_unk_m_in)[i]]\n",
    "                            else:\n",
    "                                i +=1\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                if decision =='m':\n",
    "                    morph = str(input(\"which morph:\"))\n",
    "                    try:\n",
    "                        del eb_unk_m_in[morph]\n",
    "                        eb_unk_in[word].remove(morph) \n",
    "                    except:\n",
    "                        pass\n",
    "                if decision =='a':\n",
    "                    try:\n",
    "                        examples[word].append(sent)\n",
    "                    except:\n",
    "                        pass\n",
    "                if decision =='q':\n",
    "                    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbfe935a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def how_to_add(word):\n",
    "    \"\"\"\n",
    "    p = plain, add without conjurgating or changing\n",
    "    c = conjugate as normal verb\n",
    "    x = noun that adds x when plural\n",
    "    s = noun that add s when plural\n",
    "    iv = inner verb. There is a verb in the expression that should be \n",
    "        conjurgated\n",
    "    \"\"\"\n",
    "    extras_morphs = []\n",
    "    extras_morphs.append(word)\n",
    "    \n",
    "    \n",
    "    gs = goslate.Goslate()\n",
    "\n",
    "    default_conjugator = mlconjug3.Conjugator(language='fr')\n",
    "\n",
    "    decision = str(input(word+\"\\nc\\nx\\ns\\niv\"))\n",
    "    \n",
    "    if decision == 'c':\n",
    "\n",
    "        try:\n",
    "            temp = []\n",
    "            test_verb = default_conjugator.conjugate(word)\n",
    "            all_conjugated_forms = test_verb.iterate()\n",
    "            for item in all_conjugated_forms:\n",
    "                if item not in temp:\n",
    "                    temp.append(item[len(item)-1])\n",
    "            extras_morphs.extend(list(set(temp)))\n",
    "        except:\n",
    "            pass\n",
    "    elif decision == 'x':\n",
    "        extras_morphs.append(lemma+'x')\n",
    "    elif decision == 's':\n",
    "        extras_morphs.append(lemma+'s')\n",
    "    elif decision == 'iv':\n",
    "        morph = str(input(\"which morph:\"))\n",
    "        \n",
    "\n",
    "        temp = []\n",
    "        test_verb = default_conjugator.conjugate(morph)\n",
    "        all_conjugated_forms = test_verb.iterate()\n",
    "        for item in all_conjugated_forms:\n",
    "            if item not in temp:\n",
    "                temp.append(item[len(item)-1])\n",
    "\n",
    "        for variant in list(set(temp)):\n",
    "            if variant is not None:\n",
    "                new_word = word.replace(morph,variant)\n",
    "                extras_morphs.append(new_word)\n",
    "\n",
    "    return extras_morphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "bd4719e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "For eb_unk words: search through all articles, return \n",
    "sentences containing any words for unknown vocabulary. \n",
    "\n",
    "return dict object, key is lemma, then values are morphs found\n",
    "and array of examples. \n",
    "\"\"\"\n",
    "\n",
    "def find_unks(webpages,eb_unk_mo,prev_examples):\n",
    "    \n",
    "    output = {}\n",
    "    for webpage in webpages:\n",
    "        try:\n",
    "\n",
    "            request=urllib.request.Request(webpage,None,headers) #The assembled request\n",
    "            response = urllib.request.urlopen(request)\n",
    "            data = response.read()\n",
    "            contents = text_from_html(data)\n",
    "\n",
    "            contents_array = sent_tokenize(contents)\n",
    "\n",
    "            for line in contents_array:\n",
    "                tokenized = word_tokenize(line, language='french')\n",
    "                for word in tokenized:\n",
    "                    if word.lower() in eb_unk_mo:\n",
    "\n",
    "                        if word.lower() in prev_examples:\n",
    "                            if line not in prev_examples[word.lower()]:\n",
    "                                if word.lower() not in output:\n",
    "                                    output[eb_unk_mo[word.lower()]] = {} #key will be lemma\n",
    "                                    output[eb_unk_mo[word.lower()]]['morph'] = []\n",
    "                                    output[eb_unk_mo[word.lower()]]['sents'] = []\n",
    "\n",
    "                                output[eb_unk_mo[word.lower()]]['morph'].append(word.lower())\n",
    "                                output[eb_unk_mo[word.lower()]]['sents'].append(line)\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "    return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "928c99a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_text(webpage,text=None,print_word_lvl=False):\n",
    "\n",
    "\n",
    "    #try:\n",
    "    if text is None:\n",
    "        request=urllib.request.Request(webpage,None,headers) #The assembled request\n",
    "        response = urllib.request.urlopen(request)\n",
    "        data = response.read()\n",
    "        contents = text_from_html(data)\n",
    "    else:\n",
    "        contents = webpage\n",
    "\n",
    "    line_array = []\n",
    "    percent_array = []\n",
    "    contents_array = sent_tokenize(contents)\n",
    "\n",
    "    disallowed_words = set()\n",
    "    total_words = 0\n",
    "    unknown_words = 0\n",
    "    lines = []\n",
    "    unknowns = []\n",
    "    \n",
    "    for line in contents_array:\n",
    "        \n",
    "        line_total = 0\n",
    "        line_unks = 0\n",
    "        tokenized = word_tokenize(line, language='french')\n",
    "\n",
    "        unk_str = \"\"\n",
    "        for word in tokenized:\n",
    "            total_words +=1\n",
    "            line_total +=1\n",
    "\n",
    "            if not (bool(re.search(\"[0-9]\", word)) or\\\n",
    "                    bool(re.search(\"[A-Z]\", word)) or\\\n",
    "                    bool(re.search(\"[.,\\/#!$%\\^&\\*;:{}=\\-_`~()«»]\", word)) or\\\n",
    "                    word.lower() in vocab['white_listed']):\n",
    "                if not ((word.lower() in vocab['white_listed']) or (word.lower() in vocab['black_listed'])):\n",
    "                    unk_str += '\"'+word.lower()+'\"'+\", \"\n",
    "\n",
    "                disallowed_words.add(word.lower())\n",
    "                unknown_words +=1\n",
    "                line_unks +=1\n",
    "\n",
    "\n",
    "        line_array.append(line)\n",
    "        line_percent = (line_total-line_unks)/line_total\n",
    "        percent_array.append(line_percent)\n",
    "\n",
    "        if len(unk_str)>0:\n",
    "            unk_str = unk_str[0:len(unk_str)-1]\n",
    "\n",
    "        unknowns.append(unk_str)\n",
    "\n",
    "    if print_word_lvl and total_words>0:\n",
    "        print(\"word-level % known = \"+str((1-(unknown_words/total_words))*100))\n",
    "\n",
    "    return_pd = pd.DataFrame(list(zip(line_array,percent_array,unknowns)))\n",
    "    return_pd.columns = [\"line\",\"line_percent\",\"unk_words\"]\n",
    "\n",
    "    with open(path+'unknown_french_dad_list.txt',\"w\") as outfile:\n",
    "        outfile.write(str(list(disallowed_words)))\n",
    "\n",
    "    return return_pd\n",
    "    \n",
    "    #except:\n",
    "    #    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaeef7b3",
   "metadata": {},
   "source": [
    "##Dad section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "595f5cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/Users/elyebliss/Desktop/Vocabulary/language_learning/vocab_dfs/\"\n",
    "source_file = \"french_dad.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6f33f82c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13376\n"
     ]
    }
   ],
   "source": [
    "#read from existing .json formats\n",
    "with open(path+source_file, \"r\") as path_in:\n",
    "    vocab = json.loads(path_in.read())\n",
    "vocab['white_listed'] = set(vocab['white_listed'])\n",
    "vocab['black_listed'] = set(vocab['black_listed'])\n",
    "print(len(vocab['white_listed']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a0de92d",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking Journal en francais facile\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['https://savoirs.rfi.fr//fr/podcasts/le-journal-en-fran%C3%A7ais-facile/20221123-kiev-priv%C3%A9e-d-%C3%A9lectricit%C3%A9-cyberattaque-au-parlement-europ%C3%A9en-l-affaire-adrien-quatennens',\n",
       " 'https://savoirs.rfi.fr//fr/podcasts/le-journal-en-fran%C3%A7ais-facile/20221122-france-les-magistrats-sont-en-gr%C3%A8ve-kherson-bombard%C3%A9e-par-les-russes-le-mondial-de-foot',\n",
       " 'https://savoirs.rfi.fr//fr/podcasts/le-journal-en-fran%C3%A7ais-facile/20221121-mondial-de-foot-tortures-en-ukraine-chol%C3%A9ra-au-liban-ch%C3%B4mage-en-france',\n",
       " 'https://savoirs.rfi.fr//fr/podcasts/le-journal-en-fran%C3%A7ais-facile/20221118-la-cor%C3%A9e-du-nord-tire-un-missile-la-cop27-black-ad-remporte-le-prix-d%C3%A9couvertes-rfi',\n",
       " 'https://savoirs.rfi.fr//fr/podcasts/le-journal-en-fran%C3%A7ais-facile/20221117-600-prisonniers-lib%C3%A9r%C3%A9s-en-birmanie-l-ukraine-bombard%C3%A9e-que-fait-emmanuel-macron-%C3%A0-bangkok',\n",
       " 'https://savoirs.rfi.fr//fr/podcasts/le-journal-en-fran%C3%A7ais-facile/20221116-la-fus%C3%A9e-artemis-d%C3%A9colle-lula-%C3%A0-la-cop27-un-missile-tombe-sur-la-pologne',\n",
       " 'https://savoirs.rfi.fr//fr/podcasts/le-journal-en-fran%C3%A7ais-facile/20221115-la-russie-bombarde-l-ukraine-8-milliards-d-habitants-sur-terre-donald-trump-de-retour',\n",
       " 'https://savoirs.rfi.fr//fr/podcasts/le-journal-en-fran%C3%A7ais-facile/20221114-joe-biden-et-xi-jinping-%C3%A0-bali-attentat-%C3%A0-istanbul-emmanuel-macron-veut-sanctionner-l-iran',\n",
       " 'https://savoirs.rfi.fr//fr/podcasts/le-journal-en-fran%C3%A7ais-facile/20221111-l-ocean-viking-est-arriv%C3%A9-%C3%A0-toulon-la-cop-27-sadio-man%C3%A9-ira-au-qatar']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "            \n",
    "##Journal en francais facile:\n",
    "print(\"Checking Journal en francais facile\")\n",
    "parser = 'html.parser'  # or 'lxml' (preferred) or 'html5lib', if installed\n",
    "#request=urllib.request.Request('https://savoirs.rfi.fr/en/apprendre-enseigner/langue-fran%C3%A7aise/journal-en-francais-facile',None,headers)\n",
    "request=urllib.request.Request('https://francaisfacile.rfi.fr/fr/podcasts/journal-en-fran%c3%a7ais-facile/',None,headers)\n",
    "\n",
    "resp = urllib.request.urlopen(request)\n",
    "soup = bs4.BeautifulSoup(resp, parser, from_encoding=resp.info().get_param('charset'))\n",
    "\n",
    "\n",
    "pages = []\n",
    "for link in soup.find_all('a', href=True):\n",
    "    if '/fr/podcasts/le-journal-en-fran%C3%A7ais-facile/' in str(link['href']):\n",
    "        #print(str(link['href']))\n",
    "        str_page = 'https://savoirs.rfi.fr/'+str(link['href'])\n",
    "        #str_page = 'https://savoirs.rfi.fr/en/apprendre-enseigner/langue-francaise/journal-en-francais-facile-'+str(re.search(r\"[0-9]{8}.*\",str(link['href'])).group(0))\n",
    "        if str_page not in pages:\n",
    "            pages.append(str_page)\n",
    "\n",
    "pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9959f849",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path+\"text_input_web_dad_fr.txt\",\"r\") as infile:\n",
    "    textfile = infile.read()\n",
    "filtered_art = filter_text(textfile,text= True)\n",
    "#display(filtered_art)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "27e5eb9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "154"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_unks = set()\n",
    "\n",
    "with open(path+'unknown_french_dad_list.txt',\"r\") as input_file:\n",
    "    new_words = input_file.read()\n",
    "    new_words = re.sub('\\n',\"\",new_words)\n",
    "    new_words = new_words.split(',')\n",
    "    new_words = [re.sub(\"^['|\\\"]|^\\['|['|\\\"]$|'\\]$|\\[\",\"\",line.strip().lower()) for line in new_words] #update regex\n",
    "    for word in new_words:\n",
    "        all_unks.add(word)\n",
    "len(all_unks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3fcd0a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_unks=list(all_unks)\n",
    "unk_df = pd.DataFrame(all_unks)\n",
    "unk_df.columns = ['word']\n",
    "unk_df['status'] = pd.Series(['' for word in all_unks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3f7f884f",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c44cf59ba59d4e93a2d6423875304240",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MitoWidget(analysis_data_json='{\"analysisName\": \"id-xgjqdbiwry\", \"analysisToReplay\": null, \"code\": [], \"stepSu…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mitosheet.sheet(unk_df, analysis_to_replay=\"id-xgjqdbiwry\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "28a99605",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_ints = [0,8,15,18,20,23,26,40,42,43,50,51,52,53,56,58,62,68,70,72,76,78,80,81,82,83,84,85,89,92,86,87,99,101,104,107,108,110,116,119,125,126,129,132,133,138,140,142,143,151,152]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8cffa16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in add_ints:\n",
    "    # Set a cell value in status\n",
    "    unk_df.at[i, 'status'] = 'k'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7f2053fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "translations = []\n",
    "for i in range(0,len(unk_df)):\n",
    "    try:\n",
    "        translations.append(ts.google(unk_df.word.iloc[i]))\n",
    "    except:\n",
    "        translations.append('')\n",
    "unk_df['translations']=translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e91e62e3",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "304d97ed5769420fa3c18599b893dca5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MitoWidget(analysis_data_json='{\"analysisName\": \"id-ianxevwttg\", \"analysisToReplay\": null, \"code\": [], \"stepSu…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mitosheet.sheet(unk_df, analysis_to_replay=\"id-ianxevwttg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "710c5ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51\n"
     ]
    }
   ],
   "source": [
    "add_words = list(unk_df.word[unk_df['status']=='k'])\n",
    "print(len(add_words))\n",
    "for word in add_words:\n",
    "    vocab['white_listed'].add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e0060836",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_art = filter_text(textfile,text= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "60edfe4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_art['theme']=['' for line in filtered_art.line]\n",
    "filtered_art = filtered_art[['theme','unk_words','line','line_percent']]\n",
    "with open(path+'dad_choose_themes.xlsx','wb') as outfile:\n",
    "    filtered_art.to_excel(outfile, encoding='utf-8',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3dbd955b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path+'dad_choose_themes.xlsx','rb') as infile:\n",
    "    filtered_art = pd.read_excel(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "dfef5bc9",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><div id=2c17bb6a-d116-46aa-ab16-2ce3f2dd02a4 style=\"display:none; background-color:#9D6CFF; color:white; width:200px; height:30px; padding-left:5px; border-radius:4px; flex-direction:row; justify-content:space-around; align-items:center;\" onmouseover=\"this.style.backgroundColor='#BA9BF8'\" onmouseout=\"this.style.backgroundColor='#9D6CFF'\" onclick=\"window.commands?.execute('create-mitosheet-from-dataframe-output');\">See Full Dataframe in Mito</div> <script> if (window.commands?.hasCommand('create-mitosheet-from-dataframe-output')) document.getElementById('2c17bb6a-d116-46aa-ab16-2ce3f2dd02a4').style.display = 'flex' </script> <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>theme</th>\n",
       "      <th>line</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>football</td>\n",
       "      <td>Et puis nous parlerons de sport de la Coupe du monde de football.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>football</td>\n",
       "      <td>L'Allemagne se fait surprendre par le Japon.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>football</td>\n",
       "      <td>Les Allemands battus par les Japonais.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>football</td>\n",
       "      <td>Un match de foot marqué par une image.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>football</td>\n",
       "      <td>Une image très forte.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>ukraine</td>\n",
       "      <td>La guerre en Ukraine et de nouveaux bombardements russes, bombardements qui ont fait au moins trois morts et six blessés à Kiev.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>ukraine</td>\n",
       "      <td>Correspondant régional de Radio France Internationale, Stéphane, trois centrales nucléaires ukrainiennes ont été déconnectées, conséquence?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>ukraine</td>\n",
       "      <td>Plus d'électricité dans une grande partie du pays.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>ukraine</td>\n",
       "      <td>Et puis à noter Adrien que le président français va une nouvelle fois parler au président russe\\n\\nEmmanuel Macron annonce qu'il aura un « contact direct » avec Vladimir Poutine, et ce, « dans les prochains jours » pour notamment parler de la centrale nucléaire de Zaporijia centrale nécessaire pour avoir accès à l'électricité en Ukraine.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>depute</td>\n",
       "      <td>En France, un député est accusé par sa femme d'être très violent.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table></div>"
      ],
      "text/plain": [
       "       theme  \\\n",
       "13  football   \n",
       "14  football   \n",
       "15  football   \n",
       "17  football   \n",
       "18  football   \n",
       "19  football   \n",
       "20  football   \n",
       "87  football   \n",
       "88  football   \n",
       "66  COP        \n",
       "67  COP        \n",
       "70  COP        \n",
       "77  COP        \n",
       "81  COP        \n",
       "21  ukraine    \n",
       "25  ukraine    \n",
       "26  ukraine    \n",
       "37  ukraine    \n",
       "11  depute     \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                   line  \n",
       "13  Et puis nous parlerons de sport de la Coupe du monde de football.                                                                                                                                                                                                                                                                                    \n",
       "14  L'Allemagne se fait surprendre par le Japon.                                                                                                                                                                                                                                                                                                         \n",
       "15  Les Allemands battus par les Japonais.                                                                                                                                                                                                                                                                                                               \n",
       "17  Un match de foot marqué par une image.                                                                                                                                                                                                                                                                                                               \n",
       "18  Une image très forte.                                                                                                                                                                                                                                                                                                                                \n",
       "19  Les Allemands ont posé une main sur leur bouche avant le début du match.                                                                                                                                                                                                                                                                             \n",
       "20  On vous explique pourquoi à la fin du journal.                                                                                                                                                                                                                                                                                                       \n",
       "87  Ce soir Belgique - Canada.                                                                                                                                                                                                                                                                                                                           \n",
       "88  C'est à 19 h. Enfin une image forte au Mondial, juste avant le début du match entre l'Allemagne et le Japon.                                                                                                                                                                                                                                         \n",
       "66  Les conséquences du réchauffement climatique sur la planète maintenant.                                                                                                                                                                                                                                                                              \n",
       "67  Exemple en Australie.                                                                                                                                                                                                                                                                                                                                \n",
       "70  Et c'est un rapport, une étude gouvernementale qui le dit aujourd'hui, les climatologues, ceux qui étudient le climat, sont très inquiets pour le futur.                                                                                                                                                                                             \n",
       "77  Il pleut beaucoup moins, en particulier dans le sud ouest du pays.                                                                                                                                                                                                                                                                                   \n",
       "81  « Les choses sont en train de changer avec des objectifs plus ambitieux de réduction des émissions de gaz carbonique.                                                                                                                                                                                                                                \n",
       "21  La guerre en Ukraine et de nouveaux bombardements russes, bombardements qui ont fait au moins trois morts et six blessés à Kiev.                                                                                                                                                                                                                     \n",
       "25  Correspondant régional de Radio France Internationale, Stéphane, trois centrales nucléaires ukrainiennes ont été déconnectées, conséquence?                                                                                                                                                                                                          \n",
       "26  Plus d'électricité dans une grande partie du pays.                                                                                                                                                                                                                                                                                                   \n",
       "37  Et puis à noter Adrien que le président français va une nouvelle fois parler au président russe\\n\\nEmmanuel Macron annonce qu'il aura un « contact direct » avec Vladimir Poutine, et ce, « dans les prochains jours » pour notamment parler de la centrale nucléaire de Zaporijia centrale nécessaire pour avoir accès à l'électricité en Ukraine.  \n",
       "11  En France, un député est accusé par sa femme d'être très violent.                                                                                                                                                                                                                                                                                    "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "themes = set(filtered_art.theme[filtered_art['theme']!='NaN'])\n",
    "output = pd.DataFrame(columns=['theme','line'])\n",
    "for theme in themes:\n",
    "    temp = filtered_art[['theme','line']][filtered_art['theme']==theme]\n",
    "    output = pd.concat([output,temp])\n",
    "with open(path+'dad_article.xlsx','wb') as outfile:\n",
    "    output.to_excel(outfile, encoding='utf-8',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "89e13847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13427\n"
     ]
    }
   ],
   "source": [
    "#write to .json formats\n",
    "df = vocab\n",
    "df['white_listed'] = list(df['white_listed'])\n",
    "df['black_listed'] = list(df['black_listed'])\n",
    "with open(path+source_file, \"w\") as outfile:\n",
    "    json.dump(df,outfile)\n",
    "print(len(vocab['white_listed']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "246924c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13427\n"
     ]
    }
   ],
   "source": [
    "#read from existing .json formats\n",
    "with open(path+source_file, \"r\") as path_in:\n",
    "    vocab = json.loads(path_in.read())\n",
    "vocab['white_listed'] = set(vocab['white_listed'])\n",
    "vocab['black_listed'] = set(vocab['black_listed'])\n",
    "print(len(vocab['white_listed']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8730fa01",
   "metadata": {},
   "source": [
    "##Filter for lessons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bb42b118",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path+\"text_input_dad_fr.txt\",\"r\") as infile:\n",
    "    textfile = infile.read()\n",
    "filtered_art = filter_text(textfile,text= True)\n",
    "#display(filtered_art)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd5f6cb8",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "cou\n",
      "Known =k\n",
      "15 remaining\n",
      "poitrine\n",
      "Known =k\n",
      "14 remaining\n",
      "rassuré\n",
      "Known =k\n",
      "13 remaining\n",
      "vaste\n",
      "Known =k\n",
      "12 remaining\n",
      "tatouages\n",
      "Known =k\n",
      "11 remaining\n",
      "souvenirs\n",
      "Known =k\n",
      "10 remaining\n",
      "maladie\n",
      "Known =k\n",
      "9 remaining\n",
      "seins\n",
      "Known =k\n",
      "8 remaining\n",
      "aliments\n",
      "Known =k\n",
      "7 remaining\n",
      "réuni\n",
      "Known =k\n",
      "6 remaining\n",
      "genoux\n",
      "Known =k\n",
      "5 remaining\n",
      "bizarre\n",
      "Known =kk\n",
      "got 1\n",
      "4 remaining\n",
      "colline\n",
      "Known =k\n",
      "3 remaining\n",
      "–\n",
      "Known =kk\n",
      "got 2\n",
      "2 remaining\n",
      "indigènes\n",
      "Known =k\n",
      "1 remaining\n",
      "lors\n",
      "Known =k\n",
      "0 remaining\n"
     ]
    }
   ],
   "source": [
    "with open(path+'unknown_french_dad_list.txt',\"r\") as input_file:\n",
    "    new_words = input_file.read()\n",
    "    new_words = re.sub('\\n',\"\",new_words)\n",
    "    new_words = new_words.split(',')\n",
    "    new_words = [re.sub(\"^['|\\\"]|^\\['|['|\\\"]$|'\\]$\",\"\",line.strip().lower()) for line in new_words] #update regex\n",
    "#new_words\n",
    "print(len(new_words))\n",
    "known_manual = get_known(new_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "700c2106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "print(len(known_manual))\n",
    "for word in known_manual:\n",
    "    vocab['white_listed'].add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8e25d54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#known_lines_only = list(set(list(filtered_art.knowns[filtered_art.knowns != \"...\"])))\n",
    "#known_lines_only = sorted(known_lines_only,key=len)\n",
    "filtered_art = filter_text(textfile,text= True)\n",
    "known_lines_only = list(filtered_art.line[filtered_art['line_percent']==1.0])\n",
    "known_lines_only = sorted(known_lines_only,key=len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d1a4f648",
   "metadata": {},
   "outputs": [],
   "source": [
    "#export knowns: [filtered_art.knowns != \"...\"]\n",
    "#known_list = list(filtered_art.knowns)\n",
    "with open(path+\"known_lines_dad_fr.txt\",\"w\") as outfile:\n",
    "    for line in known_lines_only:\n",
    "        outfile.write(line+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "02a15572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13376\n"
     ]
    }
   ],
   "source": [
    "#write to .json formats\n",
    "df = vocab\n",
    "df['white_listed'] = list(df['white_listed'])\n",
    "df['black_listed'] = list(df['black_listed'])\n",
    "with open(path+source_file, \"w\") as outfile:\n",
    "    json.dump(df,outfile)\n",
    "print(len(vocab['white_listed']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "c7f67a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13199\n"
     ]
    }
   ],
   "source": [
    "#read from existing .json formats\n",
    "with open(path+source_file, \"r\") as path_in:\n",
    "    vocab = json.loads(path_in.read())\n",
    "vocab['white_listed'] = set(vocab['white_listed'])\n",
    "vocab['black_listed'] = set(vocab['black_listed'])\n",
    "print(len(vocab['white_listed']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befef513",
   "metadata": {},
   "source": [
    "##EB section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d3965943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining vocab size = 2322\n",
      "Total learned = 43\n"
     ]
    }
   ],
   "source": [
    "#read eb unknown files from .json formats\n",
    "\"\"\"\n",
    "Remaining vocab size = 2325\n",
    "Total learned = 31\n",
    "\"\"\"\n",
    "with open(path+'eb_unk.json', \"r\") as infile:\n",
    "    eb_unk = json.loads(infile.read())\n",
    "print(\"Remaining vocab size = \"+str(len(eb_unk)))\n",
    "\n",
    "with open(path+'eb_unk_mo.json', \"r\") as infile:\n",
    "    eb_unk_mo = json.loads(infile.read())\n",
    "\n",
    "with open(path+'eb_learned.json', \"r\") as infile:\n",
    "    eb_learned= json.loads(infile.read())\n",
    "print(\"Total learned = \"+str(len(eb_learned)))\n",
    "\n",
    "with open(path+'eb_unk_examples.json', \"r\") as infile:\n",
    "    eb_unk_examples= json.loads(infile.read())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3fef5819",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "riposte\n",
      "c\n",
      "x\n",
      "s\n",
      "ivs\n",
      "davantage \n",
      "c\n",
      "x\n",
      "s\n",
      "iv\n",
      "peiné\n",
      "c\n",
      "x\n",
      "s\n",
      "ivs\n",
      "briguer\n",
      "c\n",
      "x\n",
      "s\n",
      "ivc\n",
      "rien n’est joué \n",
      "c\n",
      "x\n",
      "s\n",
      "iv\n",
      "ne date pas d’hier\n",
      "c\n",
      "x\n",
      "s\n",
      "iv\n",
      "agacer \n",
      "c\n",
      "x\n",
      "s\n",
      "ivc\n",
      "autochtones\n",
      "c\n",
      "x\n",
      "s\n",
      "iv\n",
      "d'aplomb\n",
      "c\n",
      "x\n",
      "s\n",
      "iv\n",
      "âpre\n",
      "c\n",
      "x\n",
      "s\n",
      "ivs\n",
      "Remaining vocab size = 2332\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "space = plain, add without conjurgating or changing\n",
    "c = conjugate as normal verb\n",
    "x = noun that adds x when plural\n",
    "s = noun that add s when plural\n",
    "iv = inner verb. There is a verb in the expression that should be \n",
    "    conjurgated\n",
    "\"\"\"\n",
    "with open(path+\"fr_eb_extra.txt\",\"r\") as infile:\n",
    "    extras_lemmas = infile.read().split('\\n')\n",
    "\n",
    "for lemma in extras_lemmas:\n",
    "    extras_morphs = how_to_add(lemma)\n",
    "    \n",
    "    eb_unk[lemma] = extras_morphs\n",
    "    for morph in extras_morphs:        \n",
    "        if morph not in eb_unk_mo:\n",
    "            eb_unk_mo[morph] = lemma\n",
    "\n",
    "print(\"Remaining vocab size = \"+str(len(eb_unk)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad05d46d",
   "metadata": {},
   "source": [
    "Get larger list of pages to search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "07860d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152\n"
     ]
    }
   ],
   "source": [
    "websites = set()\n",
    "with open(path+'viewed_websites_fr.txt',\"r\") as infile:\n",
    "    for line in infile.read().split('\\n'):\n",
    "        websites.add(line)\n",
    "print(len(websites))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "502d6ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1471214f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking France24\n"
     ]
    }
   ],
   "source": [
    "##France24\n",
    "print(\"Checking France24\")\n",
    "parser = 'html.parser'  # or 'lxml' (preferred) or 'html5lib', if installed\n",
    "#request=urllib.request.Request('https://savoirs.rfi.fr/en/apprendre-enseigner/langue-fran%C3%A7aise/journal-en-francais-facile',None,headers)\n",
    "request=urllib.request.Request('https://www.france24.com/fr/',None,headers)\n",
    "\n",
    "resp = urllib.request.urlopen(request)\n",
    "soup = bs4.BeautifulSoup(resp, parser, from_encoding=resp.info().get_param('charset'))\n",
    "\n",
    "\n",
    "\n",
    "for link in soup.find_all('a', href=True):\n",
    "    str_ver = str(link['href'])\n",
    "    \n",
    "    #get regex such that links start with /fr/ and don't end\n",
    "    #with /\n",
    "    if bool(re.search('^\\/fr\\/.*',str_ver)) and (str_ver[len(str_ver)-1]!='/'):\n",
    "        pages.append('https://www.france24.com'+str_ver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "64184cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking 20minutes\n"
     ]
    }
   ],
   "source": [
    "##20minutes\n",
    "print(\"Checking 20minutes\")\n",
    "parser = 'html.parser'  # or 'lxml' (preferred) or 'html5lib', if installed\n",
    "#request=urllib.request.Request('https://savoirs.rfi.fr/en/apprendre-enseigner/langue-fran%C3%A7aise/journal-en-francais-facile',None,headers)\n",
    "request=urllib.request.Request('https://www.20minutes.fr/',None,headers)\n",
    "\n",
    "resp = urllib.request.urlopen(request)\n",
    "soup = bs4.BeautifulSoup(resp, parser, from_encoding=resp.info().get_param('charset'))\n",
    "\n",
    "for link in soup.find_all('a', href=True):\n",
    "    str_ver = str(link['href'])\n",
    "    \n",
    "    final_segment = str_ver.split('/')[len(str_ver.split('/'))-1]\n",
    "    #articles have https:// at the beginning and a date in the last segment\n",
    "    if ('https://www.20minutes.fr' in str_ver)\\\n",
    "    and bool(re.search('[0-9]',final_segment)):\n",
    "        pages.append(str_ver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e77fee63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking TV5Monde\n"
     ]
    }
   ],
   "source": [
    "##TV5Monde\n",
    "print(\"Checking TV5Monde\")\n",
    "parser = 'html.parser'  # or 'lxml' (preferred) or 'html5lib', if installed\n",
    "#request=urllib.request.Request('https://savoirs.rfi.fr/en/apprendre-enseigner/langue-fran%C3%A7aise/journal-en-francais-facile',None,headers)\n",
    "request=urllib.request.Request('https://www.tv5monde.com/',None,headers)\n",
    "\n",
    "resp = urllib.request.urlopen(request)\n",
    "soup = bs4.BeautifulSoup(resp, parser, from_encoding=resp.info().get_param('charset'))\n",
    "\n",
    "for link in soup.find_all('a', href=True):\n",
    "    if '/info/' in str(link['href']):\n",
    "        #print(str(link['href']))\n",
    "        #str_page = 'https://savoirs.rfi.fr/'+str(link['href'])\n",
    "        #str_page = 'https://savoirs.rfi.fr/en/apprendre-enseigner/langue-francaise/journal-en-francais-facile-'+str(re.search(r\"[0-9]{8}.*\",str(link['href'])).group(0))\n",
    "        if str(link['href']) not in pages:\n",
    "            pages.append(str(link['href']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8f866a5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "151"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages = list(set(pages))\n",
    "len(pages)\n",
    "#add in set_difference line here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3f5b4891",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path+'viewed_websites_fr.txt',\"w\") as outfile:\n",
    "    for line in websites:\n",
    "        outfile.write(line+'\\n')      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7ef8ba92",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = find_unks(pages,eb_unk_mo,eb_unk_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bd6920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemma: val\n",
      "Morph: ['val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val', 'val']\n",
      "New examples: \n",
      "Imprimer          À lire aussi            17/11/22 |                             Liberté  Rennes : Chris Ames, le sans-abri « ovni » qui a choisi de vivre sous un…              22/09/22 |                             DRAME  Melun : Un SDF retrouvé mort par un passant dans un parking              17/11/22 |                             Drame  Une sans-abri meurt dans un parking souterrain de Toulouse              08/11/22 |                             PRECARITE  A Nice, le premier centre d’accueil de jour pour les femmes sans abri…              04/11/22 |                             STOCKAGE  Angers : Des casiers vont permettre aux SDF de mettre leurs affaires à…         Plus  d’actu      Précédent    1 sur 2     Suivant       Dans la région            18:01 |                             SOLIDARITE  Des doudounes pour les SDF à Paris, spécialement conçues par Monoprix              17:08 |                             Transport  Les jeunes usagères des transports, victimes n°1 des atteintes sexuelles              14:16 |                             bataille judiciaire  La Mairie de Paris envisage une plainte contre Gabriel Attal         Plus  d’articles                                          Retourner en haut de la page        Annonces Légales   Annonces légales  Allier  Alpes Maritimes  Bas Rhin  Bouches du Rhône  Côtes d'Armor  Finistère  Haut Rhin  Hauts de Seine  Loire  Loire Atlantique  Loiret  Maine et Loire  Marne  Paris  Seine Maritime  Val d'Oise  Val de Marne  Var  Vaucluse  Yvelines      Services   Distribution  Horoscope  Horoscope chinois  Conjugaison  Recette de cuisine  Résultats des élections  Maison  Le Journal des Seniors  Bébés et Mamans  Infolocale  Sportune   Codes promo   Codes promo  Code promo AliExpress  Code promo Carrefour  Code promo Dell  Code promo Vistaprint  Code promo Black Friday     20 Minutes   Recrutement  La 21 e Minute  20 Minutes Média  Panel des lecteurs  20 Minutes Livres  Charte   Jeux   Jeux gratuits  Mots fléchés  Solitaire  Puzzle  Mots croisés  Sudoku      Réseaux sociaux        Facebook  2,9M         Twitter  2.4M         Instagram  320K         Youtube  27K         Dailymotion  9,8K         Flipboard  114K      Newsletters  Votre résumé de l'actualité à lire tous les matins     Applications mobiles  Découvrez nos applications             La politique RSE de 20 Minutes est certifiée par le label Silver d’Ecovadis        Plan du site  Notre charte  Journal PDF  Archives  Publicité  Mentions légales  CGU  Protection des données personnelles  Gestion des cookies  Nous contacter    Choix de consentement  © Copyright  20 Minutes  - La fréquentation de 20 Minutes est certifiée par l’ACPM\n",
      "options : k,m,a,q\n",
      "Imprimer          À lire aussi            17/11/22 |                             Liberté  Rennes : Chris Ames, le sans-abri « ovni » qui a choisi de vivre sous un…              22/09/22 |                             DRAME  Melun : Un SDF retrouvé mort par un passant dans un parking              17/11/22 |                             Drame  Une sans-abri meurt dans un parking souterrain de Toulouse              08/11/22 |                             PRECARITE  A Nice, le premier centre d’accueil de jour pour les femmes sans abri…              04/11/22 |                             STOCKAGE  Angers : Des casiers vont permettre aux SDF de mettre leurs affaires à…         Plus  d’actu      Précédent    1 sur 2     Suivant       Dans la région            18:01 |                             SOLIDARITE  Des doudounes pour les SDF à Paris, spécialement conçues par Monoprix              17:08 |                             Transport  Les jeunes usagères des transports, victimes n°1 des atteintes sexuelles              14:16 |                             bataille judiciaire  La Mairie de Paris envisage une plainte contre Gabriel Attal         Plus  d’articles                                          Retourner en haut de la page        Annonces Légales   Annonces légales  Allier  Alpes Maritimes  Bas Rhin  Bouches du Rhône  Côtes d'Armor  Finistère  Haut Rhin  Hauts de Seine  Loire  Loire Atlantique  Loiret  Maine et Loire  Marne  Paris  Seine Maritime  Val d'Oise  Val de Marne  Var  Vaucluse  Yvelines      Services   Distribution  Horoscope  Horoscope chinois  Conjugaison  Recette de cuisine  Résultats des élections  Maison  Le Journal des Seniors  Bébés et Mamans  Infolocale  Sportune   Codes promo   Codes promo  Code promo AliExpress  Code promo Carrefour  Code promo Dell  Code promo Vistaprint  Code promo Black Friday     20 Minutes   Recrutement  La 21 e Minute  20 Minutes Média  Panel des lecteurs  20 Minutes Livres  Charte   Jeux   Jeux gratuits  Mots fléchés  Solitaire  Puzzle  Mots croisés  Sudoku      Réseaux sociaux        Facebook  2,9M         Twitter  2.4M         Instagram  320K         Youtube  27K         Dailymotion  9,8K         Flipboard  114K      Newsletters  Votre résumé de l'actualité à lire tous les matins     Applications mobiles  Découvrez nos applications             La politique RSE de 20 Minutes est certifiée par le label Silver d’Ecovadis        Plan du site  Notre charte  Journal PDF  Archives  Publicité  Mentions légales  CGU  Protection des données personnelles  Gestion des cookies  Nous contacter    Choix de consentement  © Copyright  20 Minutes  - La fréquentation de 20 Minutes est certifiée par l’ACPM\n",
      "options : k,m,a,q\n",
      "Plus  d’actu                                          Retourner en haut de la page        Annonces Légales   Annonces légales  Allier  Alpes Maritimes  Bas Rhin  Bouches du Rhône  Côtes d'Armor  Finistère  Haut Rhin  Hauts de Seine  Loire  Loire Atlantique  Loiret  Maine et Loire  Marne  Paris  Seine Maritime  Val d'Oise  Val de Marne  Var  Vaucluse  Yvelines      Services   Distribution  Horoscope  Horoscope chinois  Conjugaison  Recette de cuisine  Résultats des élections  Maison  Le Journal des Seniors  Bébés et Mamans  Infolocale  Sportune   Codes promo   Codes promo  Code promo AliExpress  Code promo Carrefour  Code promo Dell  Code promo Vistaprint  Code promo Black Friday     20 Minutes   Recrutement  La 21 e Minute  20 Minutes Média  Panel des lecteurs  20 Minutes Livres  Charte   Jeux   Jeux gratuits  Mots fléchés  Solitaire  Puzzle  Mots croisés  Sudoku      Réseaux sociaux        Facebook  2,9M         Twitter  2.4M         Instagram  320K         Youtube  27K         Dailymotion  9,8K         Flipboard  114K      Newsletters  Votre résumé de l'actualité à lire tous les matins     Applications mobiles  Découvrez nos applications             La politique RSE de 20 Minutes est certifiée par le label Silver d’Ecovadis        Plan du site  Notre charte  Journal PDF  Archives  Publicité  Mentions légales  CGU  Protection des données personnelles  Gestion des cookies  Nous contacter    Choix de consentement  © Copyright  20 Minutes  - La fréquentation de 20 Minutes est certifiée par l’ACPM\n",
      "options : k,m,a,q\n",
      "Plus  d’actu                                          Retourner en haut de la page        Annonces Légales   Annonces légales  Allier  Alpes Maritimes  Bas Rhin  Bouches du Rhône  Côtes d'Armor  Finistère  Haut Rhin  Hauts de Seine  Loire  Loire Atlantique  Loiret  Maine et Loire  Marne  Paris  Seine Maritime  Val d'Oise  Val de Marne  Var  Vaucluse  Yvelines      Services   Distribution  Horoscope  Horoscope chinois  Conjugaison  Recette de cuisine  Résultats des élections  Maison  Le Journal des Seniors  Bébés et Mamans  Infolocale  Sportune   Codes promo   Codes promo  Code promo AliExpress  Code promo Carrefour  Code promo Dell  Code promo Vistaprint  Code promo Black Friday     20 Minutes   Recrutement  La 21 e Minute  20 Minutes Média  Panel des lecteurs  20 Minutes Livres  Charte   Jeux   Jeux gratuits  Mots fléchés  Solitaire  Puzzle  Mots croisés  Sudoku      Réseaux sociaux        Facebook  2,9M         Twitter  2.4M         Instagram  320K         Youtube  27K         Dailymotion  9,8K         Flipboard  114K      Newsletters  Votre résumé de l'actualité à lire tous les matins     Applications mobiles  Découvrez nos applications             La politique RSE de 20 Minutes est certifiée par le label Silver d’Ecovadis        Plan du site  Notre charte  Journal PDF  Archives  Publicité  Mentions légales  CGU  Protection des données personnelles  Gestion des cookies  Nous contacter    Choix de consentement  © Copyright  20 Minutes  - La fréquentation de 20 Minutes est certifiée par l’ACPM\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "options : k,m,a,q\n",
      "Imprimer                                              Retourner en haut de la page        Annonces Légales   Annonces légales  Allier  Alpes Maritimes  Bas Rhin  Bouches du Rhône  Côtes d'Armor  Finistère  Haut Rhin  Hauts de Seine  Loire  Loire Atlantique  Loiret  Maine et Loire  Marne  Paris  Seine Maritime  Val d'Oise  Val de Marne  Var  Vaucluse  Yvelines      Services   Distribution  Horoscope  Horoscope chinois  Conjugaison  Recette de cuisine  Résultats des élections  Maison  Le Journal des Seniors  Bébés et Mamans  Infolocale  Sportune   Codes promo   Codes promo  Code promo AliExpress  Code promo Carrefour  Code promo Dell  Code promo Vistaprint  Code promo Black Friday     20 Minutes   Recrutement  La 21 e Minute  20 Minutes Média  Panel des lecteurs  20 Minutes Livres  Charte   Jeux   Jeux gratuits  Mots fléchés  Solitaire  Puzzle  Mots croisés  Sudoku      Réseaux sociaux        Facebook  2,9M         Twitter  2.4M         Instagram  320K         Youtube  27K         Dailymotion  9,8K         Flipboard  114K      Newsletters  Votre résumé de l'actualité à lire tous les matins     Applications mobiles  Découvrez nos applications             La politique RSE de 20 Minutes est certifiée par le label Silver d’Ecovadis        Plan du site  Notre charte  Journal PDF  Archives  Publicité  Mentions légales  CGU  Protection des données personnelles  Gestion des cookies  Nous contacter    Choix de consentement  © Copyright  20 Minutes  - La fréquentation de 20 Minutes est certifiée par l’ACPM\n",
      "options : k,m,a,q\n",
      "Imprimer                                              Retourner en haut de la page        Annonces Légales   Annonces légales  Allier  Alpes Maritimes  Bas Rhin  Bouches du Rhône  Côtes d'Armor  Finistère  Haut Rhin  Hauts de Seine  Loire  Loire Atlantique  Loiret  Maine et Loire  Marne  Paris  Seine Maritime  Val d'Oise  Val de Marne  Var  Vaucluse  Yvelines      Services   Distribution  Horoscope  Horoscope chinois  Conjugaison  Recette de cuisine  Résultats des élections  Maison  Le Journal des Seniors  Bébés et Mamans  Infolocale  Sportune   Codes promo   Codes promo  Code promo AliExpress  Code promo Carrefour  Code promo Dell  Code promo Vistaprint  Code promo Black Friday     20 Minutes   Recrutement  La 21 e Minute  20 Minutes Média  Panel des lecteurs  20 Minutes Livres  Charte   Jeux   Jeux gratuits  Mots fléchés  Solitaire  Puzzle  Mots croisés  Sudoku      Réseaux sociaux        Facebook  2,9M         Twitter  2.4M         Instagram  320K         Youtube  27K         Dailymotion  9,8K         Flipboard  114K      Newsletters  Votre résumé de l'actualité à lire tous les matins     Applications mobiles  Découvrez nos applications             La politique RSE de 20 Minutes est certifiée par le label Silver d’Ecovadis        Plan du site  Notre charte  Journal PDF  Archives  Publicité  Mentions légales  CGU  Protection des données personnelles  Gestion des cookies  Nous contacter    Choix de consentement  © Copyright  20 Minutes  - La fréquentation de 20 Minutes est certifiée par l’ACPM\n",
      "options : k,m,a,q\n",
      "Imprimer          À lire aussi            16/11/22 |                             CONVERSION  « Reste un peu » : Gad Elmaleh fait part de sa crise de foi à ses parents…              15/11/22 |                             COACHING  « Le Défi de Noël » : Comment redonner à une footballeuse l’envie de…              15/11/22 |                             MIGRANTS  « Les Engagés » : Comment Benjamin Lavernhe s'est retrouvé à venir en aide…              15/11/22 |                             COMEDIE  « Les Femmes du square » : Comment des nounous exploitées s'y prennent…              15/11/22 |                             FIN DE VIE  « Plus que jamais » : Gaspard Ulliel une dernière fois confronté au choix…         Plus  d’actu      Précédent    1 sur 2     Suivant                                           Retourner en haut de la page        Annonces Légales   Annonces légales  Allier  Alpes Maritimes  Bas Rhin  Bouches du Rhône  Côtes d'Armor  Finistère  Haut Rhin  Hauts de Seine  Loire  Loire Atlantique  Loiret  Maine et Loire  Marne  Paris  Seine Maritime  Val d'Oise  Val de Marne  Var  Vaucluse  Yvelines      Services   Distribution  Horoscope  Horoscope chinois  Conjugaison  Recette de cuisine  Résultats des élections  Maison  Le Journal des Seniors  Bébés et Mamans  Infolocale  Sportune   Codes promo   Codes promo  Code promo AliExpress  Code promo Carrefour  Code promo Dell  Code promo Vistaprint  Code promo Black Friday     20 Minutes   Recrutement  La 21 e Minute  20 Minutes Média  Panel des lecteurs  20 Minutes Livres  Charte   Jeux   Jeux gratuits  Mots fléchés  Solitaire  Puzzle  Mots croisés  Sudoku      Réseaux sociaux        Facebook  2,9M         Twitter  2.4M         Instagram  320K         Youtube  27K         Dailymotion  9,8K         Flipboard  114K      Newsletters  Votre résumé de l'actualité à lire tous les matins     Applications mobiles  Découvrez nos applications             La politique RSE de 20 Minutes est certifiée par le label Silver d’Ecovadis        Plan du site  Notre charte  Journal PDF  Archives  Publicité  Mentions légales  CGU  Protection des données personnelles  Gestion des cookies  Nous contacter    Choix de consentement  © Copyright  20 Minutes  - La fréquentation de 20 Minutes est certifiée par l’ACPM\n",
      "options : k,m,a,qq\n",
      "Lemma: treillis\n",
      "Morph: ['treillis']\n",
      "New examples: \n",
      "Ces groupes disposent néanmoins toujours de combattants en treillis dans le nord de l’Irak, s’apparentant à des « réservistes » qui s’entraînent au maniement des armes.\n",
      "options : k,m,a,q\n",
      "Lemma: rédaction\n",
      "Morph: ['rédaction', 'rédaction', 'rédaction', 'rédaction', 'rédaction', 'rédaction', 'rédaction', 'rédaction', 'rédaction', 'rédaction', 'rédaction', 'rédaction', 'rédaction', 'rédaction', 'rédaction', 'rédaction', 'rédaction', 'rédaction', 'rédaction', 'rédaction', 'rédaction', 'rédaction', 'rédaction', 'rédaction', 'rédaction', 'rédaction', 'rédaction', 'rédaction', 'rédaction', 'rédaction', 'rédaction', 'rédaction', 'rédaction', 'rédaction', 'rédaction', 'rédaction', 'rédaction', 'rédaction', 'rédaction', 'rédaction']\n",
      "New examples: \n",
      "La Charte de 20 Minutes est donc un code éthique pour notre communauté qui dépasse la seule activité de la rédaction, sa responsabilité morale ou encore la traditionnelle déontologie professionnelle des journalistes.\n",
      "options : k,m,a,qa\n",
      "Pour la rédaction du journal, ce code éthique s’impose à tous les journalistes, au personnel associé à la rédaction et à ses cadres dirigeants.\n",
      "options : k,m,a,qa\n",
      "A cette fin notamment, la rédaction recourt à certains outils de l’écriture inclusive.\n"
     ]
    }
   ],
   "source": [
    "review_out(out,eb_learned,eb_unk,eb_unk_mo,eb_unk_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "75667e05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining vocab size = 2322\n",
      "Total learned = 43\n"
     ]
    }
   ],
   "source": [
    "#write to eb unknown files to .json formats\n",
    "\"\"\"\n",
    "Remaining vocab size = 2322\n",
    "Total learned = 43\n",
    "\"\"\"\n",
    "with open(path+'eb_unk.json', \"w\") as outfile:\n",
    "    json.dump(eb_unk,outfile)\n",
    "print(\"Remaining vocab size = \"+str(len(eb_unk)))\n",
    "\n",
    "with open(path+'eb_unk_mo.json', \"w\") as outfile:\n",
    "    json.dump(eb_unk_mo,outfile)\n",
    "\n",
    "with open(path+'eb_learned.json', \"w\") as outfile:\n",
    "    json.dump(eb_learned,outfile)\n",
    "print(\"Total learned = \"+str(len(eb_learned)))\n",
    "\n",
    "with open(path+'eb_unk_examples.json', \"w\") as outfile:\n",
    "    json.dump(eb_unk_examples,outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5856e8a8",
   "metadata": {},
   "source": [
    "### Code no longer in use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "2fed8e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_line_percents(webpage):\n",
    "    request=urllib.request.Request(webpage,None,headers) #The assembled request\n",
    "    response = urllib.request.urlopen(request)\n",
    "    data = response.read()\n",
    "    contents = text_from_html(data)\n",
    "\n",
    "    known_array = []\n",
    "    unk_array = []\n",
    "    contents_array = sent_tokenize(contents)\n",
    "\n",
    "    lines = []\n",
    "\n",
    "    for line in contents_array:\n",
    "        tokenized = word_tokenize(line, language='french')\n",
    "\n",
    "        line_total = 0\n",
    "        line_unks = 0\n",
    "        for word in tokenized:\n",
    "\n",
    "            line_total +=1\n",
    "\n",
    "            if not (bool(re.search(\"[0-9]\", word)) or\\\n",
    "                    bool(re.search(\"[A-Z]\", word)) or\\\n",
    "                    bool(re.search(\"[.,\\/#!$%\\^&\\*;:{}=\\-_`~()«»]\", word)) or\\\n",
    "                    word.lower() in vocab['white_listed']):\n",
    "\n",
    "                    line_unks +=1\n",
    "\n",
    "            lines.append(100*(line_total-line_unks)/line_total)\n",
    "    return(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92a7174",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if (start is not None) and (stop is not None):\n",
    "    contents_array=contents_array[max(start,0):min(stop,len(contents_array))]\n",
    "\n",
    "with open(path+'unknown_french_dad_list.txt',\"w\") as outfile:\n",
    "    outfile.write(str(list(disallowed_words)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "760cea2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining vocab size = 2327\n"
     ]
    }
   ],
   "source": [
    "##add in extras\n",
    "with open(\"/Users/elyebliss/Desktop/Vocabulary/vocab_dfs/fr_eb_extra.txt\",\"r\") as infile:\n",
    "    extras_lemmas = infile.read().split('\\n')\n",
    "\n",
    "gs = goslate.Goslate()\n",
    "\n",
    "default_conjugator = mlconjug3.Conjugator(language='fr')\n",
    "\n",
    "\n",
    "for lemma in extras_lemmas:\n",
    "    \n",
    "    decision = how_to_add(lemma)\n",
    "    \n",
    "    if lemma not in eb_unk:\n",
    "        eb_unk[lemma] = []\n",
    "        \n",
    "    extras_morphs = []\n",
    "    extras_morphs.append(lemma)\n",
    "    try:\n",
    "        #if it's an infinitive, add all conjurgations\n",
    "        test_verb = default_conjugator.conjugate(lemma)\n",
    "        all_conjugated_forms = test_verb.iterate()\n",
    "        for item in all_conjugated_forms:\n",
    "            if item not in extras_morphs:\n",
    "                extras_morphs.append(item[len(item)-1])\n",
    "        extras_morphs = list(set(extras_morphs))\n",
    "    except:\n",
    "        \n",
    "        #if not an expression:\n",
    "        if len(lemma.split(' '))==1:\n",
    "            #add plural nouns\n",
    "            if lemma[len(lemma)-1]=='u':\n",
    "                extras_morphs.append(lemma+'x')\n",
    "            else:\n",
    "                extras_morphs.append(lemma+'s')\n",
    "\n",
    "    eb_unk[lemma] = extras_morphs\n",
    "    for morph in extras_morphs:        \n",
    "        if morph not in eb_unk_mo:\n",
    "            eb_unk_mo[morph] = lemma\n",
    "\n",
    "print(\"Remaining vocab size = \"+str(len(eb_unk)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99a19293",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/elyebliss/Desktop/Vocabulary/vocab_dfs/known_french_dad_list.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Upload from download\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mknown_french_dad_list.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m input_file:\n\u001b[1;32m      3\u001b[0m     new_words \u001b[38;5;241m=\u001b[39m input_file\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m      4\u001b[0m     new_words \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,new_words)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/elyebliss/Desktop/Vocabulary/vocab_dfs/known_french_dad_list.txt'"
     ]
    }
   ],
   "source": [
    "#Upload from download\n",
    "with open(path+'known_french_dad_list.txt',\"r\") as input_file:\n",
    "    new_words = input_file.read()\n",
    "    new_words = re.sub('\\n',\"\",new_words)\n",
    "    new_words = new_words.split(',')\n",
    "    new_words = [line.replace('\"',\"\").strip().lower() for line in new_words]\n",
    "    print(\"daily catch in word count:\")\n",
    "    print(len(new_words))\n",
    "    \n",
    "    for line in new_words:   \n",
    "        vocab['white_listed'].add(line.replace('\"',\"\").strip().lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "564b37a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "to do:\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2ccd6a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "##INPUT-OUTPUT\n",
    "#vocab list:\n",
    "with open(\"/Users/elyebliss/Desktop/Vocabulary/vocab_dfs/dad_whitelisted.csv\",\"r\") as infile:\n",
    "    whitelisted_lemmas = infile.read()\n",
    "\n",
    "\n",
    "##VARIABLES\n",
    "vocab_all = set()\n",
    "gs = goslate.Goslate()\n",
    "\n",
    "default_conjugator = mlconjug3.Conjugator(language='fr')\n",
    "\n",
    "\n",
    "for line in whitelisted_lemmas.split('\\n'):\n",
    "    if len(line) > 0:\n",
    "        #print(line)\n",
    "        vocab = line.lower().strip()\n",
    "        vocab_all.add(vocab)\n",
    "        try:\n",
    "            #if it's an infinitive, add all conjurgations\n",
    "            test_verb = default_conjugator.conjugate(vocab)\n",
    "            all_conjugated_forms = test_verb.iterate()\n",
    "            for item in all_conjugated_forms:\n",
    "                \n",
    "                vocab_all.add(item[len(item)-1])\n",
    "                \n",
    "            \n",
    "        except:\n",
    "            #might be a noun, add plural\n",
    "            vocab_all.add(add_noun(vocab))\n",
    "#len(whitelisted_lemmas.split('\\n'))            \n",
    "#pp.pprint(vocab_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b1a92ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {}\n",
    "vocab['white_listed'] = list(vocab_all)\n",
    "vocab['black_listed'] = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5169625e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#used for creating eb data\n",
    "\n",
    "with open(path+'eb_unk_lemmas.csv','r') as infile:\n",
    "    eb_unk_lemmas = pd.read_csv(infile)\n",
    "with open(path+'eb_unk_morph.csv','r') as infile:\n",
    "    eb_unk_morph = pd.read_csv(infile)\n",
    "    \n",
    "eb_unk = {}\n",
    "for word in eb_unk_lemmas.lemme:\n",
    "    eb_unk[word] = []\n",
    "\n",
    "    for index, row in eb_unk_morph.iterrows():\n",
    "        if row.lemme == word:\n",
    "            eb_unk[word].append(row.ortho)\n",
    "eb_unk_mo = {}\n",
    "for lemma in eb_unk.keys():\n",
    "    for morph in eb_unk[lemma]:\n",
    "        eb_unk_mo[morph] = lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "2562e60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "eb_learned = {}\n",
    "eb_unk_examples = {}\n",
    "for word in eb_unk.keys():\n",
    "    eb_unk_examples[word] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "e313700c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#restore to backups\n",
    "eb_learned = eb_learned_backup\n",
    "eb_unk = eb_unk_backup\n",
    "eb_unk_mo = eb_unk_mo_backup\n",
    "eb_unk_examples = eb_unk_examples"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
